@techreport{Kingma,
abstract = {The framework of normalizing flows provides a general strategy for flexible vari-ational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.},
archivePrefix = {arXiv},
arxivId = {1606.04934v2},
author = {Kingma, Diederik P and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
eprint = {1606.04934v2},
file = {:C$\backslash$:/Users/nayopu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma et al. - Unknown - Improved Variational Inference with Inverse Autoregressive Flow.pdf:pdf},
title = {{Improved Variational Inference with Inverse Autoregressive Flow}}
}
@article{Borji2018,
abstract = {Generative models, in particular generative adversarial networks (GANs), have received significant attention recently. A number of GAN variants have been proposed and have been utilized in many applications. Despite large strides in terms of theoretical progress, evaluating and comparing GANs remains a daunting task. While several measures have been introduced, as of yet, there is no consensus as to which measure best captures strengths and limitations of models and should be used for fair model comparison. As in other areas of computer vision and machine learning, it is critical to settle on one or few good measures to steer the progress in this field. In this paper, I review and critically discuss more than 24 quantitative and 5 qualitative measures for evaluating generative models with a particular emphasis on GAN-derived models. I also provide a set of 7 desiderata followed by an evaluation of whether a given measure or a family of measures is compatible with them.},
archivePrefix = {arXiv},
arxivId = {1802.03446},
author = {Borji, Ali},
eprint = {1802.03446},
file = {:C$\backslash$:/Users/nayopu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Borji - 2018 - Pros and Cons of GAN Evaluation Measures.pdf:pdf},
journal = {Computer Vision and Image Understanding},
keywords = {Deep learning,Evaluation,Generative adversarial nets,Generative models,Neural networks},
month = {feb},
pages = {41--65},
publisher = {Academic Press Inc.},
title = {{Pros and Cons of GAN Evaluation Measures}},
url = {http://arxiv.org/abs/1802.03446},
volume = {179},
year = {2018}
}
@techreport{Xiao,
abstract = {This paper addresses the mode collapse for generative adversarial networks (GANs). We view modes as a geometric structure of data distribution in a metric space. Under this geometric lens, we embed subsamples of the dataset from an arbitrary metric space into th{\`{e}} 2 space, while preserving their pairwise distance distribution. Not only does this metric embedding determine the dimensionality of the latent space automatically, it also enables us to construct a mixture of Gaussians to draw latent space random vectors. We use the Gaussian mixture model in tandem with a simple augmentation of the objective function to train GANs. Every major step of our method is supported by theoretical analysis, and our experiments on real and synthetic data confirm that the generator is able to produce samples spreading over most of the modes while avoiding unwanted samples, outperforming several recent GAN variants on a number of metrics and offering new features.},
author = {Xiao, Chang and Zhong, Peilin and Zheng, Changxi},
file = {:C$\backslash$:/Users/nayopu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiao, Zhong, Zheng - Unknown - BourGAN Generative Networks with Metric Embeddings.pdf:pdf},
title = {{BourGAN: Generative Networks with Metric Embeddings}}
}
@article{Koh2017,
abstract = {How can we explain the predictions of a black-box model? In this paper, we use influence functions - a classic technique from robust statistics - to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.},
archivePrefix = {arXiv},
arxivId = {1703.04730},
author = {Koh, Pang Wei and Liang, Percy},
eprint = {1703.04730},
file = {:C$\backslash$:/Users/nayopu/Downloads/3305381.3305576.pdf:pdf},
isbn = {9781510855144},
journal = {34th International Conference on Machine Learning, ICML 2017},
pages = {2976--2987},
title = {{Understanding black-box predictions via influence functions}},
volume = {4},
year = {2017}
}
@techreport{Hara2019,
abstract = {Data cleansing is a typical approach used to improve the accuracy of machine learning models, which, however, requires extensive domain knowledge to identify the influential instances that affect the models. In this paper, we propose an algorithm that can suggest influential instances without using any domain knowledge. With the proposed method, users only need to inspect the instances suggested by the algorithm, implying that users do not need extensive knowledge for this procedure, which enables even non-experts to conduct data cleansing and improve the model. The existing methods require the loss function to be convex and an optimal model to be obtained, which is not always the case in modern machine learning. To overcome these limitations, we propose a novel approach specifically designed for the models trained with stochastic gradient descent (SGD). The proposed method infers the influential instances by retracing the steps of the SGD while incorporating intermediate models computed in each step. Through experiments, we demonstrate that the proposed method can accurately infer the influential instances. Moreover, we used MNIST and CIFAR10 to show that the models can be effectively improved by removing the influential instances suggested by the proposed method.},
archivePrefix = {arXiv},
arxivId = {1906.08473},
author = {Hara, Satoshi and Nitanda, Atsushi and Maehara, Takanori},
eprint = {1906.08473},
file = {:C$\backslash$:/Users/nayopu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hara, Nitanda, Maehara - Unknown - Data Cleansing for Models Trained with SGD(3).pdf:pdf;:C$\backslash$:/Users/nayopu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hara, Nitanda, Maehara - Unknown - Data Cleansing for Models Trained with SGD.pdf:pdf},
title = {{Data Cleansing for Models Trained with SGD}},
url = {http://arxiv.org/abs/1906.08473},
year = {2019}
}
@article{Miller2019,
author = {Miller, Tim and Agosto, Denise and Baillie, Emma and Cashmore, Michael and Flach, Peter and M, Daniel Le and Leake, David and Lim, Brian and Mueller, Shane and Parsons, Simon and Recio-garcia, Juan and Watson, Ian},
file = {:C$\backslash$:/Users/nayopu/Downloads/Proceedings of the IJCAI2019 Workshop on Explainable AI.pdf:pdf},
title = {{Proceedings of the IJCAI 2019 Workshop on Explainable Artificial Intelligence Edited by Rosina Weber , Drexel University Program Committee}},
year = {2019}
}
@article{Mescheder2017,
abstract = {In this paper, we analyze the numerics of common algorithms for training Generative Adversarial Networks (GANs). Using the formalism of smooth two-player games we analyze the associated gradient vector field of GAN training objectives. Our findings suggest that the convergence of current algorithms suffers due to two factors: i) presence of eigenvalues of the Jacobian of the gradient vector field with zero real-part, and ii) eigenvalues with big imaginary part. Using these findings, we design a new algorithm that overcomes some of these limitations and has better convergence properties. Experimentally, we demonstrate its superiority on training common GAN architectures and show convergence on GAN architectures that are known to be notoriously hard to train.},
archivePrefix = {arXiv},
arxivId = {1705.10461},
author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
eprint = {1705.10461},
file = {:C$\backslash$:/Users/nayopu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mescheder, Nowozin, Geiger - 2017 - The Numerics of GANs(3).pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
month = {may},
pages = {1826--1836},
publisher = {Neural information processing systems foundation},
title = {{The Numerics of GANs}},
url = {http://arxiv.org/abs/1705.10461},
volume = {2017-December},
year = {2017}
}
@article{Mescheder2018,
abstract = {Recent work has shown local convergence of GAN training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. Furthermore, we discuss reg-ularization strategies that were recently proposed to stabilize GAN training. Our analysis shows that GAN training with instance noise or zero-centered gradient penalties converges. On the other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number of discriminator updates per generator update do not always converge to the equilibrium point. We discuss these results, leading us to a new explanation for the stability problems of GAN training. Based on our analysis, we extend our convergence results to more general GANs and prove local convergence for simplified gradient penalties even if the generator and data distributions lie on lower dimensional manifolds. We find these penalties to work well in practice and use them to learn high-resolution generative image models for a variety of datasets with little hyperparameter tuning.},
author = {Mescheder, Lars and Geiger, Andreas and Nowozin, Sebastian},
file = {:C$\backslash$:/Users/nayopu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mescheder, Geiger, Nowozin - 2018 - Which Training Methods for GANs do actually Converge.pdf:pdf},
title = {{Which Training Methods for GANs do actually Converge?}},
url = {http://proceedings.mlr.press/v80/mescheder18a/mescheder18a.pdf},
year = {2018}
}
@techreport{Goodfellow,
abstract = {We propose a new framework for estimating generative models via an adversar-ial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1 2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
author = {Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
file = {:C$\backslash$:/Users/nayopu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodfellow et al. - Unknown - Generative Adversarial Nets.pdf:pdf},
title = {{Generative Adversarial Nets}},
url = {http://www.github.com/goodfeli/adversarial}
}
@techreport{Mitsuo,
abstract = {Deep Neural Network (DNN) suffers from noisy labeled data because of the heavily overfitting risk. To avoid the risk, in this paper, we propose a novel sample selection framework for learning noisy samples. The core idea is to employ a "regret" minimization approach. The proposed sample selection method adaptively selects a subset of noisy-labeled training samples to minimize the regret to select noise samples. The algorithm efficiently works and performs with theoretical support. Moreover, unlike the typical approaches, the algorithm does not require any side information or learning information depending on the training settings of DNN. The experimental results demonstrate that the proposed method improves the performance of a black-box DNN with noisy labeled data.},
archivePrefix = {arXiv},
arxivId = {2003.03179v1},
author = {Mitsuo, Nariaki and Uchida, Seiichi and Suehiro, Daiki},
eprint = {2003.03179v1},
file = {:C$\backslash$:/Users/nayopu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mitsuo, Uchida, Suehiro - Unknown - No Regret Sample Selection with Noisy Labels.pdf:pdf},
keywords = {Index Terms-noisy labeled data,adaptive sample selection,regret minimization},
title = {{No Regret Sample Selection with Noisy Labels}}
}
@techreport{Khanna2019,
abstract = {Research in both machine learning and psychology suggests that salient examples can help humans to interpret learning models. To this end, we take a novel look at black box interpretation of test predictions in terms of training examples. Our goal is to ask "which training examples are most responsible for a given set of predictions"? To answer this question, we make use of Fisher kernels as the defining feature embedding of each data point, combined with Sequential Bayesian Quadrature (SBQ) for efficient selection of examples. In contrast to prior work, our method is able to seamlessly handle any sized subset of test predictions in a principled way. We theoretically analyze our approach, providing novel convergence bounds for SBQ over discrete candidate atoms. Our approach recovers the application of influence functions for interpretability as a special case yielding novel insights from this connection. We also present applications of the proposed approach to three use cases: cleaning training data, fixing mislabeled examples and data summariza-tion.},
author = {Khanna, Rajiv and Kim, Been and Ghosh, Joydeep and Koyejo, Oluwasanmi and Brain, Berkeley Google},
file = {:C$\backslash$:/Users/nayopu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Khanna et al. - 2019 - Interpreting Black Box Predictions using Fisher Kernels(2).pdf:pdf},
title = {{Interpreting Black Box Predictions using Fisher Kernels}},
year = {2019}
}

@article{cook,
author = { R.   Dennis   Cook },
title = {Detection of Influential Observation in Linear Regression},
journal = {Technometrics},
volume = {19},
number = {1},
pages = {15-18},
year  = {1977},
publisher = {Taylor & Francis},
doi = {10.1080/00401706.1977.10489493},

URL = { 
        https://doi.org/10.1080/00401706.1977.10489493
    
},
eprint = { 
        https://doi.org/10.1080/00401706.1977.10489493
    
}

}

@article{Cook1980,
abstract = {Traditionally, most of the effort in fitting full rank linear regression models has centered on the study of the presence, strength and form of relationships between the measured variables. As is now well known, least squares regression computations can be strongly influenced by a few cases, and a fitted model may more accurately reflect unusual features of those cases than the overall relationships between the variables. It is of interest, therefore, for an analyst to be able to find influential cases and, based on them, make decisions concerning their usefulness in a problem at hand. Based on an empirical influence function, we discuss methodologies for assessing the influence of individual or groups of cases on a regression problem. We conclude with an example using data from the Florida Area Cumulus Experiments (FACE) on cloud seeding. {\textcopyright} 1980 Taylor {\&} Francis Group, LLC.},
author = {Cook, R. Dennis and Weisberg, Sanford},
doi = {10.1080/00401706.1980.10486199},
issn = {15372723},
journal = {Technometrics},
keywords = {Cloud seeding,Distance measures,Linear models,Outlier tests,Residual plotting,Robustness},
mendeley-groups = {SGDInfluence},
number = {4},
pages = {495--508},
publisher = {American Statistical Association},
title = {{Characterizations of an empirical influence function for detecting influential cases in regression}},
volume = {22},
year = {1980}
}



